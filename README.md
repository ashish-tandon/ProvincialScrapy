# Provincial Scrapy ğŸ‡¨ğŸ‡¦

A comprehensive, production-ready system for scraping and tracking legislative bills across all Canadian provinces and territories. Built with resilience and scalability in mind.

## ğŸŒŸ Key Features

- **Complete Provincial Coverage**: Automated scrapers for all Canadian provinces and territories
- **Multiple Scraping Strategies**: Firecrawl API integration with intelligent fallback mechanisms
- **Real-time Dashboard**: Modern Next.js interface for viewing and managing bills
- **Robust Data Pipeline**: Queue-based scraping with retry logic and error handling
- **RESTful API**: Complete API for programmatic access to bill data
- **Advanced Search**: Full-text search across all bills with filtering
- **Automatic Updates**: Scheduled scraping with configurable intervals
- **Data Integrity**: Duplicate detection and data normalization

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js UI    â”‚â”€â”€â”€â”€â–¶â”‚  MCP Server  â”‚â”€â”€â”€â”€â–¶â”‚   NocoDB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                      â–²
                               â–¼                      â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                        â”‚    Redis     â”‚              â”‚
                        â”‚  Job Queue   â”‚              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                               â”‚                      â”‚
                               â–¼                      â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                        â”‚   Python     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  Scrapers    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **Frontend**: Next.js 14, TypeScript, Tailwind CSS, shadcn/ui
- **API Server**: Node.js, Express, Bull Queue, Winston logging
- **Database**: NocoDB (SQLite/PostgreSQL compatible)
- **Scrapers**: Python 3.10+, BeautifulSoup4, Custom base classes
- **Queue**: Redis for job management
- **External Services**: Firecrawl API (optional)

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone <repository-url>
cd ProvincialScrapy

# Set up environment
cp .env.example .env
# Edit .env with your configuration

# Start with Docker Compose
docker-compose -f docker-compose.dev.yml up -d

# Run initial scraping
docker exec provincial-scrapy-app-dev python src/unified_scraper.py

# Access services
# Frontend: http://localhost:3000
# API: http://localhost:3001
# Database: http://localhost:8080
```

## ğŸ“¦ Project Structure

```
ProvincialScrapy/
â”œâ”€â”€ frontend/              # Next.js frontend application
â”‚   â”œâ”€â”€ src/              
â”‚   â”‚   â”œâ”€â”€ app/          # App router pages
â”‚   â”‚   â”œâ”€â”€ components/   # Reusable UI components
â”‚   â”‚   â””â”€â”€ store/        # State management
â”‚   â””â”€â”€ public/           # Static assets
â”œâ”€â”€ mcp-server/           # Node.js API server
â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”œâ”€â”€ routes/           # API endpoints
â”‚   â””â”€â”€ queues/           # Job processing
â”œâ”€â”€ src/                  # Python scrapers
â”‚   â”œâ”€â”€ scrapers/         # Province-specific scrapers
â”‚   â”‚   â”œâ”€â”€ base_scraper.py    # Base class with common logic
â”‚   â”‚   â””â”€â”€ *_scraper.py       # Province scrapers
â”‚   â””â”€â”€ unified_scraper.py     # Main orchestrator
â”œâ”€â”€ data/                 # Scraped data storage
â”œâ”€â”€ nginx/                # Reverse proxy config
â””â”€â”€ docker-compose.yml    # Container orchestration
```

## ğŸ”§ Configuration

### Required Environment Variables

```env
# Firecrawl API (optional but recommended)
FIRECRAWL_API_KEY=your-api-key

# Database
NOCODB_API_TOKEN=your-token

# Security
JWT_SECRET=your-secret-key

# Redis (if external)
REDIS_HOST=redis
REDIS_PORT=6379
```

### Supported Provinces

- âœ… Ontario (Enhanced)
- âœ… British Columbia
- âœ… Alberta
- âœ… Quebec
- âœ… Saskatchewan
- âœ… Manitoba
- ğŸ”„ Nova Scotia (in progress)
- ğŸ”„ New Brunswick (in progress)
- ğŸ”„ Newfoundland and Labrador (in progress)
- ğŸ”„ Prince Edward Island (in progress)
- ğŸ”„ Northwest Territories (planned)
- ğŸ”„ Yukon (planned)
- ğŸ”„ Nunavut (planned)

## ğŸ“Š API Endpoints

### Scraping Operations
- `POST /api/scrape/all` - Trigger scraping for all provinces
- `POST /api/scrape/province/:province` - Scrape specific province
- `GET /api/scrape/status/:jobId` - Check scraping job status

### Data Access
- `GET /api/bills/:province` - Get bills by province
- `GET /api/bills/:province/:billNumber` - Get specific bill details
- `GET /api/search/bills?q=keyword` - Search across all bills
- `GET /api/stats` - Get system statistics

### Authentication
- `POST /api/auth/register` - Create new account
- `POST /api/auth/login` - User login
- `GET /api/auth/verify` - Verify JWT token

## ğŸ›  Development

### Adding New Province Scrapers

1. Create new file in `src/scrapers/`
2. Inherit from `BaseScraper`
3. Implement `scrape_current_bills()` method
4. Register in `src/scrapers/__init__.py`

Example:
```python
from .base_scraper import BaseScraper

class YukonScraper(BaseScraper):
    def __init__(self):
        super().__init__("Yukon")
        self.base_url = "https://yukonassembly.ca"
        
    def scrape_current_bills(self):
        # Implementation
        pass
```

### Running Tests

```bash
# All tests
docker exec provincial-scrapy-app-dev pytest

# Specific scraper
docker exec provincial-scrapy-app-dev pytest tests/test_ontario_scraper.py
```

## ğŸš€ Production Deployment

```bash
# Use production compose file
docker-compose -f docker-compose.prod.yml up -d

# Set up cron for regular scraping
0 */6 * * * cd /app && docker exec app python src/unified_scraper.py
```

## ğŸ“ˆ Monitoring

- Health endpoint: `GET /health`
- Logs: `docker-compose logs -f [service-name]`
- Database admin: http://localhost:8080

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Firecrawl](https://firecrawl.dev) for intelligent web scraping
- [NocoDB](https://nocodb.com) for the database layer
- All provincial legislature websites for public data access

## ğŸ“ Support

- ğŸ“§ Create an issue for bugs or feature requests
- ğŸ“– See [SETUP_INSTRUCTIONS.md](SETUP_INSTRUCTIONS.md) for detailed setup
- ğŸ’¬ Join discussions in the Issues section
